\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\providecommand \oddpage@label [2]{}
\citation{need2009next}
\citation{popejoy2016genomics}
\citation{peterson2019genome}
\citation{martin2017human}
\citation{pasaniuc2011enhanced}
\citation{zaitlen2010leveraging}
\citation{martin2017human}
\citation{vilhjalmsson2015modeling}
\citation{carlson2013generalization}
\citation{martin2019clinical}
\citation{shimodaira2000improving}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\newlabel{introduction}{{1}{1}{Introduction}{section.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Background}{1}{subsection.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Covariate Shift Correction}{1}{subsection.1.2}\protected@file@percent }
\newlabel{Covariate Shift Correction}{{1.2}{1}{Covariate Shift Correction}{subsection.1.2}{}}
\citation{shimodaira2000improving}
\citation{sheather1991reliable}
\citation{fishman2013monte}
\citation{sugiyama2010dimensionality}
\citation{sugiyama2010dimensionality}
\citation{gretton2009covariate}
\citation{sugiyama2010density}
\@writefile{toc}{\contentsline {section}{\numberline {2}Importance Estimation}{4}{section.2}\protected@file@percent }
\newlabel{Importance Estimation}{{2}{4}{Importance Estimation}{section.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Kernel Mean Matching (KMM)}{4}{subsection.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.1}Finite-order Approach}{4}{subsubsection.2.1.1}\protected@file@percent }
\newlabel{kmm_finite}{{1}{4}{Finite-order Approach}{equation.2.1}{}}
\citation{kanamori2009condition}
\citation{gretton2009covariate}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.2}Infinite-order Approach}{5}{subsubsection.2.1.2}\protected@file@percent }
\citation{smola1998learning}
\citation{tsuboi2009direct}
\citation{sugiyama2010density}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Kullback-Leibler Importance Estimation Procedure (KLIEP)}{6}{subsection.2.2}\protected@file@percent }
\newlabel{ratio appro}{{3}{6}{Kullback-Leibler Importance Estimation Procedure (KLIEP)}{equation.2.3}{}}
\newlabel{kliep_obj}{{4}{7}{Kullback-Leibler Importance Estimation Procedure (KLIEP)}{equation.2.4}{}}
\@writefile{loa}{\contentsline {algocf}{\numberline {1}{\ignorespaces KL Importance Estimation Procedure\relax }}{8}{algocf.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.1}Model Selection by Cross-validation}{8}{subsubsection.2.2.1}\protected@file@percent }
\citation{yamada2013relative}
\@writefile{loa}{\contentsline {algocf}{\numberline {2}{\ignorespaces cross-validation for KLIEP\relax }}{9}{algocf.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Unconstrained Least-square Importance Fitting (uLSIF)}{9}{subsection.2.3}\protected@file@percent }
\citation{kanamori2009condition}
\citation{tsuboi2009direct}
\newlabel{ulsif_reg}{{6}{10}{Unconstrained Least-square Importance Fitting (uLSIF)}{equation.2.6}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.1}Leave-one-out Cross-validation (LOOCV)}{10}{subsubsection.2.3.1}\protected@file@percent }
\@writefile{loa}{\contentsline {algocf}{\numberline {3}{\ignorespaces uLSIF with LOOCV (in below $*$ denotes the element-wise multiplication.)\relax }}{11}{algocf.3}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{ulsif algo}{{3}{11}{Leave-one-out Cross-validation (LOOCV)}{algocf.3}{}}
\citation{stojanov2019low}
\citation{yamada2013relative}
\citation{yamada2013relative}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Challenges}{12}{subsection.2.4}\protected@file@percent }
\newlabel{challenges}{{2.4}{12}{Challenges}{subsection.2.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces $\|\beta -\hat  {\beta }\|^{2}$ for KMM, KLIEP and uLSIF against dimensionality of variables. The squared errors of density ratio are shown on log10 scale.\relax }}{13}{figure.caption.4}\protected@file@percent }
\newlabel{l2_density_ratio.pdf}{{1}{13}{$\|\beta -\hat {\beta }\|^{2}$ for KMM, KLIEP and uLSIF against dimensionality of variables. The squared errors of density ratio are shown on log10 scale.\relax }{figure.caption.4}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Average AUC for different density ratio estimation methods over a range of feature dimensionality, values in bracket indicates the standard deviation.\relax }}{13}{table.caption.5}\protected@file@percent }
\newlabel{tab_sim}{{1}{13}{Average AUC for different density ratio estimation methods over a range of feature dimensionality, values in bracket indicates the standard deviation.\relax }{table.caption.5}{}}
\citation{wang2011dimension}
\citation{jain2000statistical}
\citation{wang2008approaches}
\citation{wang2011dimension}
\citation{barshan2011supervised}
\citation{gretton2005measuring}
\@writefile{toc}{\contentsline {section}{\numberline {3}Supervised Dimension Reduction}{14}{section.3}\protected@file@percent }
\newlabel{Supervised Dimension Reduction}{{3}{14}{Supervised Dimension Reduction}{section.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Supervised PCA}{14}{subsection.3.1}\protected@file@percent }
\newlabel{Supervised PCA}{{3.1}{14}{Supervised PCA}{subsection.3.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.1}Hilbert-Schmidt Independence Criterion (HSIC)}{15}{subsubsection.3.1.1}\protected@file@percent }
\newlabel{HSIC equation}{{7}{15}{Hilbert-Schmidt Independence Criterion (HSIC)}{equation.3.7}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.2}Supervised PCA Algorithm}{16}{subsubsection.3.1.2}\protected@file@percent }
\newlabel{SPCA}{{8}{16}{Supervised PCA Algorithm}{equation.3.8}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.3}The Dual Problem}{16}{subsubsection.3.1.3}\protected@file@percent }
\citation{ghojogh2019unsupervised}
\citation{fulton2013representation}
\@writefile{loa}{\contentsline {algocf}{\numberline {4}{\ignorespaces Dual Supervised PCA\relax }}{17}{algocf.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.4}Kernel Supervised PCA}{17}{subsubsection.3.1.4}\protected@file@percent }
\citation{ghojogh2019eigenvalue}
\citation{ghojogh2019unsupervised}
\citation{fisher1936use}
\@writefile{loa}{\contentsline {algocf}{\numberline {5}{\ignorespaces Kernel Supervised PCA\relax }}{18}{algocf.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Local Fisher Discriminant Analysis}{18}{subsection.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.1}Fisher Linear Discriminant Analysis}{18}{subsubsection.3.2.1}\protected@file@percent }
\citation{he2004locality}
\citation{belkin2003laplacian}
\citation{roweis2000nonlinear}
\citation{zelnik2005self}
\newlabel{sw}{{9}{19}{Fisher Linear Discriminant Analysis}{equation.3.9}{}}
\newlabel{sb}{{10}{19}{Fisher Linear Discriminant Analysis}{equation.3.10}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.2}Local Preserving Projection (LPP)}{19}{subsubsection.3.2.2}\protected@file@percent }
\citation{he2004locality}
\citation{sugiyama2007dimensionality}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.3}Local Fisher Discriminant Analysis (LFDA)}{20}{subsubsection.3.2.3}\protected@file@percent }
\newlabel{s}{{11}{20}{Local Fisher Discriminant Analysis (LFDA)}{equation.3.11}{}}
\newlabel{w}{{12}{20}{Local Fisher Discriminant Analysis (LFDA)}{equation.3.12}{}}
\citation{fukunaga2013introduction}
\newlabel{lfda}{{13}{21}{Local Fisher Discriminant Analysis (LFDA)}{equation.3.13}{}}
\citation{fukunaga2013introduction}
\@writefile{loa}{\contentsline {algocf}{\numberline {6}{\ignorespaces Local Linear Fisher Discriminant Analysis\relax }}{22}{algocf.6}\protected@file@percent }
\citation{sugiyama2007dimensionality}
\citation{sugiyama2007dimensionality}
\citation{friedman1989regularized}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.4}Kernel Local Linear Fisher Discriminant Analysis (KLFDA)}{23}{subsubsection.3.2.4}\protected@file@percent }
\newlabel{eq}{{14}{23}{Kernel Local Linear Fisher Discriminant Analysis (KLFDA)}{equation.3.14}{}}
\newlabel{klfda}{{17}{23}{Kernel Local Linear Fisher Discriminant Analysis (KLFDA)}{equation.3.17}{}}
\citation{lee1999learning}
\newlabel{klfda_h}{{18}{24}{Kernel Local Linear Fisher Discriminant Analysis (KLFDA)}{equation.3.18}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Supervised Non-negative Matrix Factorisation}{24}{subsection.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.1}The NMF Model}{24}{subsubsection.3.3.1}\protected@file@percent }
\citation{lee1999learning}
\citation{cai2017learning}
\newlabel{loglik}{{19}{25}{The NMF Model}{equation.3.19}{}}
\@writefile{loa}{\contentsline {algocf}{\numberline {7}{\ignorespaces Non-negative matrix factorisation\relax }}{25}{algocf.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.2}Supervised NMF (SNMF)}{25}{subsubsection.3.3.2}\protected@file@percent }
\citation{cai2017learning}
\citation{wilcoxon1992individual}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.3}Selecting the Number of Types}{27}{subsubsection.3.3.3}\protected@file@percent }
\newlabel{Choose The Number of Types}{{3.3.3}{27}{Selecting the Number of Types}{subsubsection.3.3.3}{}}
\citation{damodaran2018fast}
\citation{lee2001algorithms}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Summary}{28}{subsection.3.4}\protected@file@percent }
\citation{zelnik2005self}
\citation{sugiyama2007dimensionality}
\citation{kelleher2016efficient}
\citation{gravel2011demographic}
\citation{li2010automatic}
\@writefile{toc}{\contentsline {section}{\numberline {4}Experiment on the Artificial Dataset}{30}{section.4}\protected@file@percent }
\newlabel{Results}{{4}{30}{Experiment on the Artificial Dataset}{section.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Data}{30}{subsection.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Implementation}{30}{subsection.4.2}\protected@file@percent }
\newlabel{Implementation}{{4.2}{30}{Implementation}{subsection.4.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Results}{32}{subsection.4.3}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Model performance on the simulated dataset using the raw density-ratio estimates. Default corresponds to weights all equal to 1.\relax }}{33}{table.caption.7}\protected@file@percent }
\newlabel{simulated data res}{{2}{33}{Model performance on the simulated dataset using the raw density-ratio estimates. Default corresponds to weights all equal to 1.\relax }{table.caption.7}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Model performance on the simulated dataset using the flattened weights ($\gamma = 0.5$).\relax }}{33}{table.caption.8}\protected@file@percent }
\newlabel{simulated data res flat}{{3}{33}{Model performance on the simulated dataset using the flattened weights ($\gamma = 0.5$).\relax }{table.caption.8}{}}
\newlabel{fig:sfig1}{{\caption@xref {fig:sfig1}{ on input line 1071}}{34}{Results}{figure.caption.9}{}}
\newlabel{sub@fig:sfig1}{{}{34}{Results}{figure.caption.9}{}}
\newlabel{fig:sfig2}{{\caption@xref {fig:sfig2}{ on input line 1076}}{34}{Results}{figure.caption.9}{}}
\newlabel{sub@fig:sfig2}{{}{34}{Results}{figure.caption.9}{}}
\newlabel{fig:sfig1}{{\caption@xref {fig:sfig1}{ on input line 1081}}{34}{Results}{figure.caption.9}{}}
\newlabel{sub@fig:sfig1}{{}{34}{Results}{figure.caption.9}{}}
\newlabel{fig:sfig2}{{\caption@xref {fig:sfig2}{ on input line 1086}}{34}{Results}{figure.caption.9}{}}
\newlabel{sub@fig:sfig2}{{}{34}{Results}{figure.caption.9}{}}
\newlabel{fig:sfig1}{{\caption@xref {fig:sfig1}{ on input line 1091}}{34}{Results}{figure.caption.9}{}}
\newlabel{sub@fig:sfig1}{{}{34}{Results}{figure.caption.9}{}}
\newlabel{fig:sfig2}{{\caption@xref {fig:sfig2}{ on input line 1096}}{34}{Results}{figure.caption.9}{}}
\newlabel{sub@fig:sfig2}{{}{34}{Results}{figure.caption.9}{}}
\newlabel{fig:sfig1}{{\caption@xref {fig:sfig1}{ on input line 1101}}{34}{Results}{figure.caption.9}{}}
\newlabel{sub@fig:sfig1}{{}{34}{Results}{figure.caption.9}{}}
\newlabel{fig:sfig2}{{\caption@xref {fig:sfig2}{ on input line 1106}}{34}{Results}{figure.caption.9}{}}
\newlabel{sub@fig:sfig2}{{}{34}{Results}{figure.caption.9}{}}
\newlabel{fig:sfig1}{{\caption@xref {fig:sfig1}{ on input line 1111}}{34}{Results}{figure.caption.9}{}}
\newlabel{sub@fig:sfig1}{{}{34}{Results}{figure.caption.9}{}}
\newlabel{fig:sfig2}{{\caption@xref {fig:sfig2}{ on input line 1116}}{34}{Results}{figure.caption.9}{}}
\newlabel{sub@fig:sfig2}{{}{34}{Results}{figure.caption.9}{}}
\newlabel{fig:sfig1}{{\caption@xref {fig:sfig1}{ on input line 1121}}{34}{Results}{figure.caption.9}{}}
\newlabel{sub@fig:sfig1}{{}{34}{Results}{figure.caption.9}{}}
\newlabel{fig:sfig2}{{\caption@xref {fig:sfig2}{ on input line 1126}}{34}{Results}{figure.caption.9}{}}
\newlabel{sub@fig:sfig2}{{}{34}{Results}{figure.caption.9}{}}
\newlabel{fig:sfig2}{{\caption@xref {fig:sfig2}{ on input line 1131}}{34}{Results}{figure.caption.9}{}}
\newlabel{sub@fig:sfig2}{{}{34}{Results}{figure.caption.9}{}}
\newlabel{fig:sfig1}{{\caption@xref {fig:sfig1}{ on input line 1136}}{34}{Results}{figure.caption.9}{}}
\newlabel{sub@fig:sfig1}{{}{34}{Results}{figure.caption.9}{}}
\newlabel{fig:sfig2}{{\caption@xref {fig:sfig2}{ on input line 1141}}{34}{Results}{figure.caption.9}{}}
\newlabel{sub@fig:sfig2}{{}{34}{Results}{figure.caption.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Each mirrored histogram represents a set of weight estimates. The upper half in green indicates the raw weights, the lower half in blue indicates the exponentially-flattened weights with $\gamma = 0.5$.\relax }}{34}{figure.caption.9}\protected@file@percent }
\newlabel{sim_den}{{2}{34}{Each mirrored histogram represents a set of weight estimates. The upper half in green indicates the raw weights, the lower half in blue indicates the exponentially-flattened weights with $\gamma = 0.5$.\relax }{figure.caption.9}{}}
\citation{bycroft2018uk}
\citation{biobank2015genotyping}
\@writefile{toc}{\contentsline {section}{\numberline {5}Experiment on the UK Biobank Dataset}{35}{section.5}\protected@file@percent }
\newlabel{Data}{{5}{35}{Experiment on the UK Biobank Dataset}{section.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Data}{35}{subsection.5.1}\protected@file@percent }
\newlabel{biodata}{{5.1}{35}{Data}{subsection.5.1}{}}
\citation{canela2018atlas}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Self-reported ethnicity categories in the 406,263 UK Biobank samples.\relax }}{36}{table.caption.10}\protected@file@percent }
\newlabel{ethnicity}{{4}{36}{Self-reported ethnicity categories in the 406,263 UK Biobank samples.\relax }{table.caption.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Experiment Design}{36}{subsection.5.2}\protected@file@percent }
\newlabel{Experiment Design}{{5.2}{36}{Experiment Design}{subsection.5.2}{}}
\citation{voorman2012fast}
\newlabel{fig:sfig1}{{\caption@xref {fig:sfig1}{ on input line 1207}}{37}{Data}{figure.caption.11}{}}
\newlabel{sub@fig:sfig1}{{}{37}{Data}{figure.caption.11}{}}
\newlabel{fig:sfig2}{{\caption@xref {fig:sfig2}{ on input line 1212}}{37}{Data}{figure.caption.11}{}}
\newlabel{sub@fig:sfig2}{{}{37}{Data}{figure.caption.11}{}}
\newlabel{fig:sfig1}{{\caption@xref {fig:sfig1}{ on input line 1217}}{37}{Data}{figure.caption.11}{}}
\newlabel{sub@fig:sfig1}{{}{37}{Data}{figure.caption.11}{}}
\newlabel{fig:sfig2}{{\caption@xref {fig:sfig2}{ on input line 1222}}{37}{Data}{figure.caption.11}{}}
\newlabel{sub@fig:sfig2}{{}{37}{Data}{figure.caption.11}{}}
\newlabel{fig:sfig1}{{\caption@xref {fig:sfig1}{ on input line 1227}}{37}{Data}{figure.caption.11}{}}
\newlabel{sub@fig:sfig1}{{}{37}{Data}{figure.caption.11}{}}
\newlabel{fig:sfig2}{{\caption@xref {fig:sfig2}{ on input line 1232}}{37}{Data}{figure.caption.11}{}}
\newlabel{sub@fig:sfig2}{{}{37}{Data}{figure.caption.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces First 10 PCs of all genotypes in the UK Biobank dataset, colours and shapes of points indicate the corresponding ethnic group.\relax }}{37}{figure.caption.11}\protected@file@percent }
\newlabel{fig:fig}{{3}{37}{First 10 PCs of all genotypes in the UK Biobank dataset, colours and shapes of points indicate the corresponding ethnic group.\relax }{figure.caption.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces The design of experiment.\relax }}{39}{figure.caption.12}\protected@file@percent }
\newlabel{experiment png}{{4}{39}{The design of experiment.\relax }{figure.caption.12}{}}
\citation{higham2002computing}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Results}{40}{subsection.5.3}\protected@file@percent }
\newlabel{bio res}{{5.3}{40}{Results}{subsection.5.3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces Models' performance on the UK Biobank dataset. For each evaluation metric, the numbers in brackets indicate the standard deviation. The numbers in the first column after name of each algorithm correspond to the number of folds a model has prediction results. Due to numerical instability, for some models the LASSO did not converge on all 5 train-test splits.\relax }}{40}{table.caption.13}\protected@file@percent }
\newlabel{final table}{{5}{40}{Models' performance on the UK Biobank dataset. For each evaluation metric, the numbers in brackets indicate the standard deviation. The numbers in the first column after name of each algorithm correspond to the number of folds a model has prediction results. Due to numerical instability, for some models the LASSO did not converge on all 5 train-test splits.\relax }{table.caption.13}{}}
\citation{friedman2009glmnet}
\citation{yuan2010comparison}
\newlabel{fig:sfig1}{{\caption@xref {fig:sfig1}{ on input line 1291}}{42}{Results}{figure.caption.14}{}}
\newlabel{sub@fig:sfig1}{{}{42}{Results}{figure.caption.14}{}}
\newlabel{fig:sfig2}{{\caption@xref {fig:sfig2}{ on input line 1296}}{42}{Results}{figure.caption.14}{}}
\newlabel{sub@fig:sfig2}{{}{42}{Results}{figure.caption.14}{}}
\newlabel{fig:sfig1}{{\caption@xref {fig:sfig1}{ on input line 1301}}{42}{Results}{figure.caption.14}{}}
\newlabel{sub@fig:sfig1}{{}{42}{Results}{figure.caption.14}{}}
\newlabel{fig:sfig2}{{\caption@xref {fig:sfig2}{ on input line 1306}}{42}{Results}{figure.caption.14}{}}
\newlabel{sub@fig:sfig2}{{}{42}{Results}{figure.caption.14}{}}
\newlabel{fig:sfig1}{{\caption@xref {fig:sfig1}{ on input line 1311}}{42}{Results}{figure.caption.14}{}}
\newlabel{sub@fig:sfig1}{{}{42}{Results}{figure.caption.14}{}}
\newlabel{fig:sfig2}{{\caption@xref {fig:sfig2}{ on input line 1316}}{42}{Results}{figure.caption.14}{}}
\newlabel{sub@fig:sfig2}{{}{42}{Results}{figure.caption.14}{}}
\newlabel{fig:sfig1}{{\caption@xref {fig:sfig1}{ on input line 1321}}{42}{Results}{figure.caption.14}{}}
\newlabel{sub@fig:sfig1}{{}{42}{Results}{figure.caption.14}{}}
\newlabel{fig:sfig2}{{\caption@xref {fig:sfig2}{ on input line 1326}}{42}{Results}{figure.caption.14}{}}
\newlabel{sub@fig:sfig2}{{}{42}{Results}{figure.caption.14}{}}
\newlabel{fig:sfig1}{{\caption@xref {fig:sfig1}{ on input line 1331}}{42}{Results}{figure.caption.14}{}}
\newlabel{sub@fig:sfig1}{{}{42}{Results}{figure.caption.14}{}}
\newlabel{fig:sfig2}{{\caption@xref {fig:sfig2}{ on input line 1336}}{42}{Results}{figure.caption.14}{}}
\newlabel{sub@fig:sfig2}{{}{42}{Results}{figure.caption.14}{}}
\newlabel{fig:sfig1}{{\caption@xref {fig:sfig1}{ on input line 1341}}{42}{Results}{figure.caption.14}{}}
\newlabel{sub@fig:sfig1}{{}{42}{Results}{figure.caption.14}{}}
\newlabel{fig:sfig2}{{\caption@xref {fig:sfig2}{ on input line 1346}}{42}{Results}{figure.caption.14}{}}
\newlabel{sub@fig:sfig2}{{}{42}{Results}{figure.caption.14}{}}
\newlabel{fig:sfig2}{{\caption@xref {fig:sfig2}{ on input line 1351}}{42}{Results}{figure.caption.14}{}}
\newlabel{sub@fig:sfig2}{{}{42}{Results}{figure.caption.14}{}}
\newlabel{fig:sfig1}{{\caption@xref {fig:sfig1}{ on input line 1356}}{42}{Results}{figure.caption.14}{}}
\newlabel{sub@fig:sfig1}{{}{42}{Results}{figure.caption.14}{}}
\newlabel{fig:sfig2}{{\caption@xref {fig:sfig2}{ on input line 1361}}{42}{Results}{figure.caption.14}{}}
\newlabel{sub@fig:sfig2}{{}{42}{Results}{figure.caption.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Density plots for weight estimates. Each colour represents a train-test split, the upper half of the plots shows the weights for the White individuals (account for approximately 80\% of training samples), the lower half shows weights for the Black samples. For a clear visualisation, models SPCA + KLIEP, KSPCA + KMM, KSPCA + KLIEP and LFDA + ULSIF are each split into two subfigures with different scale on the y-axis. For SPCA + ULSIF, the plot here only shows weights that are greater than 5, this counts for about 10\% of total training samples, the rest 90\% are close to 0. Similarly for SPCA + KMM, this plot only shows the top 20\% of weight estimates, the remaining are zeroes. \relax }}{42}{figure.caption.14}\protected@file@percent }
\newlabel{final figure}{{5}{42}{Density plots for weight estimates. Each colour represents a train-test split, the upper half of the plots shows the weights for the White individuals (account for approximately 80\% of training samples), the lower half shows weights for the Black samples. For a clear visualisation, models SPCA + KLIEP, KSPCA + KMM, KSPCA + KLIEP and LFDA + ULSIF are each split into two subfigures with different scale on the y-axis. For SPCA + ULSIF, the plot here only shows weights that are greater than 5, this counts for about 10\% of total training samples, the rest 90\% are close to 0. Similarly for SPCA + KMM, this plot only shows the top 20\% of weight estimates, the remaining are zeroes. \relax }{figure.caption.14}{}}
\citation{deng2013gene}
\citation{ghosh2020supervised}
\citation{le2018supervised}
\citation{yuan2012improved}
\@writefile{toc}{\contentsline {section}{\numberline {6}Further Work}{43}{section.6}\protected@file@percent }
\newlabel{Further Work}{{6}{43}{Further Work}{section.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Conclusion}{44}{section.7}\protected@file@percent }
\newlabel{Conclusion}{{7}{44}{Conclusion}{section.7}{}}
\bibstyle{apalike}
\bibdata{reference}
\bibcite{barshan2011supervised}{{1}{2011}{{Barshan et~al.}}{{}}}
\bibcite{belkin2003laplacian}{{2}{2003}{{Belkin and Niyogi}}{{}}}
\bibcite{biobank2015genotyping}{{3}{2015}{{Biobank}}{{}}}
\bibcite{bycroft2018uk}{{4}{2018}{{Bycroft et~al.}}{{}}}
\bibcite{cai2017learning}{{5}{2017}{{Cai et~al.}}{{}}}
\bibcite{canela2018atlas}{{6}{2018}{{Canela-Xandri et~al.}}{{}}}
\bibcite{carlson2013generalization}{{7}{2013}{{Carlson et~al.}}{{}}}
\bibcite{damodaran2018fast}{{8}{2018}{{Damodaran}}{{}}}
\bibcite{deng2013gene}{{9}{2013}{{Deng and Runger}}{{}}}
\bibcite{fisher1936use}{{10}{1936}{{Fisher}}{{}}}
\bibcite{fishman2013monte}{{11}{2013}{{Fishman}}{{}}}
\bibcite{friedman2009glmnet}{{12}{2009}{{Friedman et~al.}}{{}}}
\bibcite{friedman1989regularized}{{13}{1989}{{Friedman}}{{}}}
\bibcite{fukunaga2013introduction}{{14}{2013}{{Fukunaga}}{{}}}
\bibcite{fulton2013representation}{{15}{2013}{{Fulton and Harris}}{{}}}
\bibcite{ghojogh2019unsupervised}{{16}{2019}{{Ghojogh and Crowley}}{{}}}
\bibcite{ghojogh2019eigenvalue}{{17}{2019}{{Ghojogh et~al.}}{{}}}
\bibcite{ghosh2020supervised}{{18}{2020}{{Ghosh and Kirby}}{{}}}
\bibcite{gravel2011demographic}{{19}{2011}{{Gravel et~al.}}{{}}}
\bibcite{gretton2005measuring}{{20}{2005}{{Gretton et~al.}}{{}}}
\bibcite{gretton2009covariate}{{21}{2009}{{Gretton et~al.}}{{}}}
\bibcite{he2004locality}{{22}{2004}{{He and Niyogi}}{{}}}
\bibcite{higham2002computing}{{23}{2002}{{Higham}}{{}}}
\bibcite{jain2000statistical}{{24}{2000}{{Jain et~al.}}{{}}}
\bibcite{kanamori2009condition}{{25}{2009}{{Kanamori et~al.}}{{}}}
\bibcite{kelleher2016efficient}{{26}{2016}{{Kelleher et~al.}}{{}}}
\bibcite{le2018supervised}{{27}{2018}{{Le et~al.}}{{}}}
\bibcite{lee1999learning}{{28}{1999}{{Lee and Seung}}{{}}}
\bibcite{lee2001algorithms}{{29}{2001}{{Lee and Seung}}{{}}}
\bibcite{li2010automatic}{{30}{2010}{{Li et~al.}}{{}}}
\bibcite{martin2017human}{{31}{2017}{{Martin et~al.}}{{}}}
\bibcite{martin2019clinical}{{32}{2019}{{Martin et~al.}}{{}}}
\bibcite{need2009next}{{33}{2009}{{Need and Goldstein}}{{}}}
\bibcite{pasaniuc2011enhanced}{{34}{2011}{{Pasaniuc et~al.}}{{}}}
\bibcite{peterson2019genome}{{35}{2019}{{Peterson et~al.}}{{}}}
\bibcite{popejoy2016genomics}{{36}{2016}{{Popejoy and Fullerton}}{{}}}
\bibcite{roweis2000nonlinear}{{37}{2000}{{Roweis and Saul}}{{}}}
\bibcite{sheather1991reliable}{{38}{1991}{{Sheather and Jones}}{{}}}
\bibcite{shimodaira2000improving}{{39}{2000}{{Shimodaira}}{{}}}
\bibcite{smola1998learning}{{40}{1998}{{Smola and Sch{\"o}lkopf}}{{}}}
\bibcite{stojanov2019low}{{41}{2019}{{Stojanov et~al.}}{{}}}
\bibcite{sugiyama2007dimensionality}{{42}{2007}{{Sugiyama}}{{}}}
\bibcite{sugiyama2010dimensionality}{{43}{2010a}{{Sugiyama et~al.}}{{}}}
\bibcite{sugiyama2010density}{{44}{2010b}{{Sugiyama et~al.}}{{}}}
\bibcite{tsuboi2009direct}{{45}{2009}{{Tsuboi et~al.}}{{}}}
\bibcite{vilhjalmsson2015modeling}{{46}{2015}{{Vilhj{\'a}lmsson et~al.}}{{}}}
\bibcite{voorman2012fast}{{47}{2012}{{Voorman et~al.}}{{}}}
\bibcite{wang2011dimension}{{48}{2011}{{Wang and van~der Laan}}{{}}}
\bibcite{wang2008approaches}{{49}{2008}{{Wang et~al.}}{{}}}
\bibcite{wilcoxon1992individual}{{50}{1992}{{Wilcoxon}}{{}}}
\bibcite{yamada2013relative}{{51}{2013}{{Yamada et~al.}}{{}}}
\bibcite{yuan2010comparison}{{52}{2010}{{Yuan et~al.}}{{}}}
\bibcite{yuan2012improved}{{53}{2012}{{Yuan et~al.}}{{}}}
\bibcite{zaitlen2010leveraging}{{54}{2010}{{Zaitlen et~al.}}{{}}}
\bibcite{zelnik2005self}{{55}{2005}{{Zelnik-Manor and Perona}}{{}}}
\@writefile{toc}{\contentsline {section}{\numberline {A}R code: Pre-screening}{49}{appendix.1.A}\protected@file@percent }
\newlabel{Appendix A}{{A}{49}{R code: Pre-screening}{appendix.1.A}{}}
\@writefile{toc}{\contentsline {section}{\numberline {B}R code: kernel functions and kernel parameter tuning}{53}{appendix.1.B}\protected@file@percent }
\newlabel{Appendix B}{{B}{53}{R code: kernel functions and kernel parameter tuning}{appendix.1.B}{}}
\@writefile{toc}{\contentsline {section}{\numberline {C}R code: tuning Gaussian kernel parameters}{55}{appendix.1.C}\protected@file@percent }
\newlabel{Appendix B}{{C}{55}{R code: tuning Gaussian kernel parameters}{appendix.1.C}{}}
\@writefile{toc}{\contentsline {section}{\numberline {D}R code: tuning dimension reduction methods parameters}{56}{appendix.1.D}\protected@file@percent }
\newlabel{Appendix B}{{D}{56}{R code: tuning dimension reduction methods parameters}{appendix.1.D}{}}
\@writefile{toc}{\contentsline {section}{\numberline {E}R code: supervised dimension reduction}{57}{appendix.1.E}\protected@file@percent }
\newlabel{Appendix B}{{E}{57}{R code: supervised dimension reduction}{appendix.1.E}{}}
\@writefile{toc}{\contentsline {section}{\numberline {F}R code: density ratio estimation}{61}{appendix.1.F}\protected@file@percent }
\newlabel{Appendix B}{{F}{61}{R code: density ratio estimation}{appendix.1.F}{}}
\@writefile{toc}{\contentsline {section}{\numberline {G}R code: evaluate the model performance}{63}{appendix.1.G}\protected@file@percent }
